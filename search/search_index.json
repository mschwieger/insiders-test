{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About The Site","text":"<p>This is my personal website, so all opinions are my own. I made it to better document some of the helpful information I've compiled about a wide variety of topics, but also to show off some of the projects I've done that I think are neat.</p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li> <p> Guide: macOS Setup</p> <p>Suggested programs, utilities and settings for developers getting started on Mac. Including my clean and lightweight combination of ZSH plugins and utilities.</p> <p> Read more</p> </li> <li> <p> Project: Battery Backup</p> <p>After being without power for a week, I built a ~13kwh battery backup system to cover the most important circuits in our house (fridge, network and some HVAC).</p> <p> Details</p> </li> <li> <p> Smart Home: Architecture</p> <p>A general overview of my smart home setup, powered by Home Assistant.</p> <p> Read more</p> </li> </ul>"},{"location":"#who-am-i","title":"Who Am I","text":"<p>Hi, I'm Andrew LeCody. I am a maker, nerd, and a bunch of other labels that all boil down to: I like to learn and to build stuff. Professionally I am a Senior DevOps Engineer with experience at every layer, all the way down to hardware since I started my career working in datacenters. Outside of work I'm either learning, making stuff, playing video games, or camping/hiking/offroading. I helped start the Dallas Makerspace, serving on the Board of Directors and as President for the first 5 years of the organization.</p> <p>I believe strongly in the power of collaboration, protection of human rights, and taking care of our environment.</p>"},{"location":"#social-media","title":"Social Media","text":"<p>I'm most active on Mastodon these days. You can also see what I'm up to by following me on GitHub.</p>"},{"location":"blog/2009/07/21/5-tips-on-being-an-effective-linux-admin/","title":"5 Tips On Being An Effective Linux Admin","text":"<p>Earlier this week someone did something very bad, and it got me thinking about what are some of the simple things that make someone an effective Linux admin. While I will not tell you that following my advice will make you some kind of super-admin, it will help you when dealing with Linux servers and hopefully save you and others from needless headaches. Here are my tips on being an effective Linux admin, in no particular order.</p>","tags":["apache","google","linux","raid","sudo","work"]},{"location":"blog/2009/07/21/5-tips-on-being-an-effective-linux-admin/#no1-backup-files-before-modifying-them","title":"No1. Backup Files Before Modifying Them","text":"<p>Even if you think you are some kind of Linux god, you should always make backups of critical configuration files. In my time as a Linux admin, I have seen some stupid things, but the dumbest by far is when someone edits an incredibly complex config file and does not back it up first. Oh but you are just making a quick change right, maybe just modifying one line? It does not matter, because eventually you will make a mistake and you will be glad you had the foresight to make that backup first.</p> <p>Personally, I make a copy of the file in question and append my initials to the end, so \"apache2.conf\" becomes \"apache2.conf.asl\". The beauty of this method is that other users will know who changed a file and when, because they can simply look at the modify time for the file. Genius, I know.</p> <pre><code>cp apache2.conf apache2.conf.asl\n</code></pre> <p>It is that easy! There is no reason to skip this step before modifying your configuration files.</p>","tags":["apache","google","linux","raid","sudo","work"]},{"location":"blog/2009/07/21/5-tips-on-being-an-effective-linux-admin/#no2-comment-comment-comment","title":"No2. Comment, Comment, Comment","text":"<p>I am somewhat OCD when it comes to commenting. I comment religiously for a reason: I have a terrible memory. More than once I have been looking at something I have written or changed and wondered, WTF was I thinking. Commenting allows me to remember why I did something, or what it does. This is especially important when writing scripts or programs.</p> <p>Comments are important when you are doing work on a customer's server. Every time I make a configuration change for a customer, I put in a comment with the date, why I am changing the setting, the ticket ID# for the issue and my initials. This makes it easy for the customer or a technician to find information about the particular change made if there is ever an issue with it in the future.</p>","tags":["apache","google","linux","raid","sudo","work"]},{"location":"blog/2009/07/21/5-tips-on-being-an-effective-linux-admin/#no3-documentation","title":"No3. Documentation","text":"<p>Where I work, we have some older servers that were setup by \"The UNIX God\". This particular individual was given that title by someone in the company that had almost\u00a0no *NIX knowledge. He left his mark on our systems in a number of ways that we are still paying the price for, so\u00a0I have a number of colorful names that I use to refer to him.</p> <p>Documentation is important. Imagine the following situation, it is 3am, your boss calls you in a panic, a mission critical server is down, he wants you up at the datacenter or office to take care of it right now. When you get to the datacenter, you find out the server is not down, it is dead, and I am talking power supply caught fire and killed every component. The motherboard is toast, the raid array you watched slowly build is completely shot. No worries, this is why you have backups. You will just have to load an OS on a new box, load up the right services and then restore the data.</p> <p>However, there is a problem; you do not know all the details of this server's configuration. You backed up the data and the config files, but you do not have a full image of the OS. You have no idea what specific packages, utilities and libraries your company's custom written programs require. If you documented the steps taken to install the OS and every service when the system was first setup, you are golden; you just open up the documentation and retrace your steps. If you do not have documentation, you are looking forward to a long sleepless night of setting up services and then wadding through error messages to figure out what library you have overlooked.</p> <p>Having good documentation is a lot easier then most people realize, all that's really needed is a wiki. MediaWiki is easy to setup and easy to use, it is the software that powers Wikipedia. MediaWiki only needs Apache, PHP and MySQL, and has a huge support community and wiki with documentation. Grab a copy of MediaWiki and get it installed.</p> <p>Next, create a page in your new wiki for each server, project, or cluster you setup. It does not have to be terribly complex, some of mine are simply a list of commands I ran with some basic comments about what the more obscure ones do or why they are required.</p> <p>Do not forget to keep it up-to-date! Whenever you change something big, or small, be sure you update the wiki to reflect the change.</p>","tags":["apache","google","linux","raid","sudo","work"]},{"location":"blog/2009/07/21/5-tips-on-being-an-effective-linux-admin/#no4-google","title":"No4. Google","text":"<p>This one should be obvious; Google search is your friend. It is usually very rare that you will come across an issue that no one else in the world has ever run into before. Which means that there is a good chance that no matter what is broken, someone, somewhere, has already found out the solution to the problem.</p> <p>Effectively finding what you need is hard for some people. Sometimes your search turns up useless results, nothing at all, or worse yet bad advice. Unless the answer is coming from an authoritative source, like the official documentation (there is that word again), or from someone involved with the software, be wary. You should know not only what the command or setting will do, but why it will work. If you remember to make backups, you will be just fine, even if someone gives you bad advice.</p>","tags":["apache","google","linux","raid","sudo","work"]},{"location":"blog/2009/07/21/5-tips-on-being-an-effective-linux-admin/#no5-break-things-and-then-rebuild-them","title":"No5. Break Things (and then rebuild them)","text":"<p>I am not saying you should go out and break critical infrastructure, as fun as that sounds. What you should do is setup replicas of existing systems and see if you have all the information and tools necessary to rebuild the system. If you can, without cheating, setup a similar system, then the documentation is good and you now have experience in replacing the server if it should die. If for some reason you cannot recreate the system with the available information, then something is obviously missing from the documentation, find what is missing and properly document it.</p> <p>Another fun exercise is to clone a server, and then have a friend break a configuration file or two. If you want to be thorough, have them reset all the timestamps in /etc so you cannot do something like this:</p> <pre><code>find /etc -mtime -1\n</code></pre> <p>That will show you a list of files modified in the last day, it is an incredibly useful command if you know someone has been messing with files and you want to know which files.</p> <p>That is all the advice I have for now. If you have any suggestions for an article, such as a tutorial for a specific piece of software or issue, please post a comment below.</p>","tags":["apache","google","linux","raid","sudo","work"]},{"location":"blog/2009/04/09/birth-of-a-monster/","title":"Birth of a Monster","text":"<p>Sometimes I like to stay late at work, I've got a nice desk, a decent computer, and a mini-fridge full of drinks right behind me. So even after I'm off work, I sometimes stay up here and work on side projects or just browse the web. Yesterday, I decided to do something a bit more creative, I decided to build a monster.</p> <p>We recently came into the possession of several LaCie external hard drives. Most of these are 1.6TB and can use firewire or USB to connect to a computer. The downside to these drives, is that they aren't actually one single drive, each one is an enclosure with 4 IDE drives inside it setup to use JBOD or something similar to RAID 0. If even one of those drives dies, the entire LaCie is lost. Needless to say, we don't have a lot of use for what amounts to a giant hard drive with a death wish.</p> <p>So I figured, lets see how many I can hook up to one system, then software raid them and see how fast they are. I had a dual 2.4Ghz XEON box laying around, so I threw in a bunch of firewire cards. I had to remove the entire back of the case just to fit the cards, otherwise we wouldn't be able to use the card on the far left or the far right.</p> <p></p> <p>As you can see, I also had to remove the back panel of the firewire cards, since I didn't have any half height ones to use. All in all, it makes for quite an ugly setup. Here's the front and back of the entire thing:</p> <p></p> <p></p> <p>As of right now I've got mdadm (the Linux software raid utility) building a RAID 6 out of the 1.6TB drives. When I first started building the array I was given an ETA of 1,500 minutes. For those of you that can't do the math in your head (I can't), that's 25 hours! Right now it's at about 80% complete. I'll update this article sometime tomorrow, after I've got the array formatted for ext3,\u00a0 which will probably take quite awhile.</p> <p>Update</p> <p>The raid finally finished building, in total it took just under 24 hours, it took another 45 minutes to format the partition to ext3. I ran a quick test with hdparm to see how fast the array was, the individual drives are capable of about 70MBps and the array as a whole clocks in at around 150MBps. Not great, but not too bad either, the real test will be seeing how long the array stays online, since the LaCie drives aren't exactly the most robust storage devices.</p>","tags":["funny","linux","raid","ubuntu","work"]},{"location":"blog/2009/04/14/cooling-off/","title":"Cooling Off","text":"<p>I've been running Ubuntu 9.04 beta on my desktop at home for a few weeks now, so I decided it was time to upgrade my laptop. I hoped that by upgrading, I'd be able to signifigantly reduce the time it took for my laptop to boot up. Unfortunetly, disaster stuck halfway through. My laptop overheated and locked up, leaving me with half new and half old packages. Needless to say, it did not come back online very easily. I managed to get it to a point where I could copy my data off to another computer, then I did a fresh install of Ubuntu.</p> <p>I was prepared this time though, I decided I needed a way to insure my laptop will stay cool. I grabbed a dead LaCie external hard drive (one of the spares from The Monster) and set my laptop on it. These things are HEAVY, the outer case of the drive is made of a pretty solid metal (possibly steel, it feels too heavy to be aluminum) and it works great as a makeshift heatsink.</p> <p></p> <p>As you can see, it's not the nicest looking heatsink, but it'll have to do until I can fix the overheating problem. On the plus side, Ubuntu 9.04 is running great. The laptop boots up much faster then before and I love the new \"Dust\" theme. It's the first dark colored theme for Gnome that I've found to actually work well, it's also fairly compact which is great since the laptop has a small screen.</p> <p></p> <p>Tomorrow I plan on posting a tutorial on how to I tweaked my Nagios setup at work to monitor 3000 servers every 3 minutes.</p>","tags":["funny","linux","ubuntu"]},{"location":"blog/2009/07/06/fail-why-you-should-always-use-visudo/","title":"FAIL: Why you should ALWAYS use visudo","text":"<p>It's time for a short rant about proper Linux administration. Someone, who shall not be named, manually edited the /etc/sudoers file and broke it on a critical server. In case you don't know, on Linux sudo allows you to run commands as the root (Administrator) user, and the sudoers file determines who can use sudo and what they can do with it.</p> <p>When editing the sudoers file you have to be careful, if you make a change that breaks the file you can potentially lock yourself out of being able to fix the file. That's what happened today, after the change to the sudoers file any attempt to use sudo returned this error:</p> <pre><code>&gt;&gt;&gt; sudoers file: syntax error, line 6 &lt;&lt;&lt;\n&gt;&gt;&gt; sudoers file: syntax error, line 8 &lt;&lt;&lt;\nsudo: parse error in /etc/sudoers near line 6\n</code></pre> <p>On this particular system users can only gain root access by using sudo, there is a root password but for some unfathomable reason no one knows the password. So I was tasked with rebooting the server, breaking into it and then fixing the sudoers file. Normally on Debian there's a helpful boot option that takes you right into a command prompt as root, unless of course you have a password set and don't know the password. So I resorted to the less elegant method of specifying init=/bin/bash as a boot option.</p> <p>Once I had root access I used visudo to fix the sudoers file. Why visudo? Because that's the program YOU ARE SUPPOSED TO USE. Yes I am a bit angry, because the person that broke this server should have known better. Visudo is a lovely\u00a0little program that checks the syntax of the sudoers file before you save it so that if you do something monumentally stupid you'll know about it before it becomes a problem and prevents you from getting back into the system as root.</p> <p>Later this week I'll be posting an article with my list of good Linux administration habbits you'll want to ingrain into your skull.</p> <p>So far the week is off to a great start.</p>","tags":["fail","linux","sudo","work"]},{"location":"blog/2009/07/01/firefox-35-impatiently-waiting/","title":"Firefox 3.5: Impatiently Waiting","text":"<p>In case you have been living under a rock, Firefox 3.5 came out today (well technically yesterday). I've noticed a lot of people wondering how they can get it running smoothly on Ubuntu. Never fear, after much trial and error (a lot of error) I've found what I think should be the easiest way to install the latest version of Firefox. So if you are just as impatient as I am, you can rejoice in the all the cool new features.</p> <p>First up, I used a script called Ubuntuzilla, it takes care of virtually all the dirty work. Personally, I like it because whenever Ubuntu gets around to adding Firefox 3.5 to the official repositories (I don't count Minefield)\u00a0 you can simply use Ubuntuzilla to remove your \"unofficial\" version of Firefox and use Ubuntu's packaged version again.</p> <p>Before you install Ubuntuzilla, we'll need to install some dependencies. So open up a terminal and run the following command, which will install the dependencies from Ubuntu's repository. If you don't know how to use the terminal, you probably want to stop and go read the wonderful guide at linuxcommand.org first.</p> <pre><code>sudo apt-get install libstdc++5 libnotify-bin\n</code></pre> <p>Next, you'll need to know if you are running the 32bit or 64bit version of Ubuntu. The quickest way will be to run this in your terminal:</p> <pre><code>uname -a\n</code></pre> <p>The part we're looking for is near the very end of the line. If it says \"i686\" you are using 32bit, if it says \"x86_64\" you're using 64bit. Now that you know what version of Ubuntu you are running it's time to download and install Ubuntuzilla.</p> <p>For 32bit, we run:</p> <pre><code>wget http://softlayer.dl.sourceforge.net/sourceforge/ubuntuzilla/ubuntuzilla-4.6.1-0ubuntu1-i386.deb\nsudo dpkg -i ubuntuzilla-4.6.1-0ubuntu1-i386.deb\n</code></pre> <p>For 64bit:</p> <pre><code>wget http://softlayer.dl.sourceforge.net/sourceforge/ubuntuzilla/ubuntuzilla-4.6.1-0ubuntu1-amd64.deb\nsudo dpkg -i ubuntuzilla-4.6.1-0ubuntu1-amd64.deb\n</code></pre> <p>Once it's installed, you just run this command and follow the prompts (it's pretty straightforward):</p> <pre><code>sudo ubuntuzilla.py\n</code></pre> <p>If everything went smoothly, you can open up Firefox as normal and it will be version 3.5! Of course, if you're like me and you use subpixel smoothing (known as ClearType on Windows) the fonts in Firefox might look, well, ugly. Don't worry though, we can fix it!</p> <p>NOTE: This step is COMPLETELY OPTIONAL, this is only for users who need subpixel smoothing for their fonts. We're going to create a file in our home directory called \".fonts.conf\" and put in some configuration options to tell Firefox (and other programs that don't listen to Gnome's font settings) that we want smooth, beautiful fonts. From the terminal, run the following</p> <pre><code>gedit ~/.fonts.conf\n</code></pre> <p>That will open up our text editor (gedit), now put this into the file, save it, and close gedit.</p> <pre><code>&lt;?xml version=\u201d1.0\u2033?&gt;\n&lt;!DOCTYPE fontconfig SYSTEM \u201cfonts.dtd\u201d&gt;\n&lt;fontconfig&gt;\n&lt;match target=\u201dfont\u201d &gt;\n&lt;edit mode=\u201dassign\u201d name=\u201drgba\u201d &gt;\n&lt;const&gt;rgb&lt;/const&gt;\n&lt;/edit&gt;\n&lt;/match&gt;\n&lt;match target=\u201dfont\u201d &gt;\n&lt;edit mode=\u201dassign\u201d name=\u201dhinting\u201d &gt;\n&lt;bool&gt;true&lt;/bool&gt;\n&lt;/edit&gt;\n&lt;/match&gt;\n&lt;match target=\u201dfont\u201d &gt;\n&lt;edit mode=\u201dassign\u201d name=\u201dhintstyle\u201d &gt;\n&lt;const&gt;hintfull&lt;/const&gt;\n&lt;/edit&gt;\n&lt;/match&gt;\n&lt;match target=\u201dfont\u201d &gt;\n&lt;edit mode=\u201dassign\u201d name=\u201dantialias\u201d &gt;\n&lt;bool&gt;true&lt;/bool&gt;\n&lt;/edit&gt;\n&lt;/match&gt;\n&lt;/fontconfig&gt;\n</code></pre> <p>Please note, these settings are specifically to enable full smoothing, I'm not sure what the settings would be for medium or slight smoothing. If anyone knows, please post in the comments and I'll update this post.</p> <p>If you run into any trouble, drop me a line in the comments and I'll try to help you out. Now go have fun surfing the web :)</p> <p>Update 7/12:</p> <p>Thank you Antsu for providing the settings for slight and medium smoothing.</p> <p>For slight smoothing:</p> <pre><code>&lt;match target=\u201dfont\u201d &gt;\n&lt;edit mode=\u201dassign\u201d name=\u201dhintstyle\u201d &gt;\n&lt;const&gt;hintslight&lt;/const&gt;\n&lt;/edit&gt;\n&lt;/match&gt;\n</code></pre> <p>and for medium smoothing:</p> <pre><code>&lt;match target=\u201dfont\u201d &gt;\n&lt;edit mode=\u201dassign\u201d name=\u201dhintstyle\u201d &gt;\n&lt;const&gt;hintmedium&lt;/const&gt;\n&lt;/edit&gt;\n&lt;/match&gt;\n</code></pre>","tags":["firefox","linux","sourceforge","ubuntu"]},{"location":"blog/2009/07/24/phishing-attempt-via-txt/","title":"Phishing attempt via TXT","text":"<p>Hey everyone, I was going to post this yesterday, but my internet was on the fritz. On Wednesday while at dinner I received a text message telling me to call a number to activate my credit card, I knew right away this was a phishing attempt. After dinner, I went to the Verizon store right around the corner to see if they had a number for reporting these kinds of things.</p> <p>While there, I learned two things, at least one display phone at the store received this message! Second Verizon, at least this particular store, is not aware of this phenomenon at all, except for their display, nor the implications of what this means. Here is the entire message for your viewing pleasure:</p> <p>From: MsgID2_NURM57XF@v.w Resource One Credit Union Alert: Your CARD has been DEACTIVATED. Please contact us at 214-432-5945 to REACTIVATE.</p> <p>If you are at all tech savvy you will notice a ton of signs this is not legit. We should take a look at the most obvious reasons this is total crap.</p> <ul> <li>The email address that sent the message \u201cMsgID2_NURM57XF@v.w\u201d is not real. V.W is not a real domain, no matter how much it looks like Verizon Wireless\u2019 initials that is done on purpose so anyone not critically thinking about the message would believe it is from Verizon.</li> <li>I am NOT a member of any thing called Resource One Credit Union, and as such should NOT receive any information about a credit card. How is Resource One Credit Union associated with Verizon Wireless anyways? Why would they want to use Verizon Wireless\u2019 initials as part of their email address?</li> <li>I do NOT have a credit card!</li> <li>Companies do not send this sort of information via text message. Some companies might use text messages as an informal communication method, but the vast majority of text messages are by individuals to other individuals. Even if I did have a credit card, even with Resource One Credit Union, text messages are not the preferred communication medium. Normally they just lock or close your account and you are unaware until your card it is declined. Then YOU initiate communication with the credit card company, not the other way around.</li> <li>The Verizon Wireless store\u2019s demo phone received this message. Now of course you may not have gone to Verizon\u2019s store to check this out, but I did. I cannot think of a single reason why a demo phone would receive this message; it does not have a real owner.</li> </ul> <p>Please, please if you ever receive a text message or automated phone call or email from someone you do not know, pay attention. I do not want anyone to fall prey to this sort of thing and education is simply the only way.</p> <p>Update: July 24, 2009</p> <p>Ok maybe this is not really an update because I never was able to post this yesterday, but the FTC, to stop the scam, has shut down the number. Woo hoo! The system works!</p>","tags":["education","phishing","verizon"]},{"location":"blog/2009/04/08/terrorists-dont-use-google-maps/","title":"Terrorists Don't Use Google Maps","text":"<p>A while back, we got an e-mail from a customer who was concerned that terrorists might be using her site to plan an attack on the space shuttle. Please note, this is a real ticket that was submitted to our security department by one of our customers. Identifying information has been removed and replaced with asterisks (*). See the entire story after the jump.</p> <p>From the customer:</p> <p>Posted on Nov 19 2008 10:04 AM Your assistance is requested to perhaps quiet an overactive imagination, or confirm a legitimate concern.</p> <p>Our primary domain is **********.com, we also have another domain, **********.com.</p> <p>The Kennedy Space Center is located in our county, and since I have noticed a spiking in hits on our website from the Netherlands at a time of heightened terrorist alerts, I would like to know more about where in the Netherlands these hits are coming from. The Netherlands is a known location of Arab communities, and Florida was a known location of terrorists that were involved in 9/11.</p> <p>We have on our website a calendar that utilizes Google maps and identifies our parks in clear proximity to Kennedy Space Center.</p> <p>I can think of no reason for such a consistent amount of unusual high hits from another country (unless a University were using it for \"park planning\").</p> <p>My concern is that terrorists could be using it for reference to our parks as sites to shoot missiles at Kennedy Space Center.</p> <p>I would rather this concern just be my overactive imagination, but I would like to know the source IP and location for the Netherlands hits, and any other information you feel is pertinent.</p> <p>http://**********.com/stats/ http://**********.com/stats/</p> <p>Please look at the Origin Countries and you will see what I mean. Also, the unresolved IP addresses.</p> <p>You can see how convenient our Calendar maps make it for people to find their way around here on the Space Coast:</p> <p>http://www.**********.com/calendar/events/index.php?com=location</p> <p>Please let me know ASAP. Thank you!</p> <p>Security Response:</p> <p>Posted on Nov 19 2008 10:29 AM I don't think you have anything to be concerned about, I cannot see anyone using a parks website to get information when they can just use Google maps to pull the same details. Looking at the logs, I don't see any specific pattern to the .nl traffic over any other specific ip or host in your logs. As for the physical location of the various .nl ips, the Netherland ISP's do not publish physical locations to IP lookups. You can contact the local ISPs, telfort.nl and chello.nl but I do not think they will give you any more detailed information. As for unresolved IP addresses, those are just IP addresses without a reverse lookup. That is normal. Please let us know if you have any further concerns.</p> <p>Customer:</p> <p>Posted on Nov 19 2008 12:19 PM Thank you for checking on our server stats.</p> <p>However, as convenient as Google Earth and Google maps are, without our website, Google maps don't pinpoint and cluster all of the parks, nor do they provide what our website provides: park opening/closing hours, narrative driving directions, photos, and links to add'l information in our website that help people get quickly oriented, including a list of I-95 Exit ramps, and so on.</p> <p>I would like to think a lot of Netherlands folks are planning a lot of vacations, but our Tourism Director tells me most tourists from other countries to the Space Coast come from Canada, UK, and Germany (Netherlands being about 10<sup>th</sup> on the list); or, that they are using our website for park planning, however when folks do that they usually correspond with us also.</p> <p>That leaves a big question mark as to why Netherlands hits have been consistently so high since July.</p> <p>Security Response:</p> <p>Posted on Nov 19 2008 12:58 PM More than likely, this is all innocent traffic, but if you are concerned I would suggest forwarding this information to our local FBI branch. They will be able to investigate this, and if they determine there is a legitimate threat, be able to act upon it. Please go to http://www.fbi.gov/contact/fo/fo.htm for more information.</p> <p>Customer:</p> <p>Posted on Nov 19 2008 04:49 PM Thank you. I am inclined to believe so too, but it's unusual, and I don't want to brush off something that might be regrettable later.</p> <p>In attempting to dispel any possibility, I used a Netherlands search engine and entered some strings (Nederland **********, Nederland **********) that brought up only 1 or 2 results. http://zoek.lycos.nl/ So the hit level that far exceeds Canada, UK &amp; Germany is still a mystery.</p> <p>Thanks again for your assistance.</p> <p>However, it didn't end there. A few days later our security department got a call from the FBI requesting the logs for the site, citing privacy concerns our security department declined the request. The response from the FBI was basically, \"that's fine, we're pretty sure this is a non-issue.\"</p> <p>So far we haven't had any more crazy tickets from that customer.</p>","tags":["funny","google","security","work"]},{"location":"blog/2009/04/20/tweaking-nagios-for-performance/","title":"Tweaking Nagios For Performance","text":"<p>The company I work for has about 3,000 servers that need to be monitored in our Dallas datacenter. For the past few years we've been using a fairly standard Nagios setup. If you don't take the time to really learn Nagios and tweak the config files it'll run fairly well, until you are monitoring more then a few hundred servers. The reason that Nagios slows down when checking 300+ servers is that it stores all state/check information in a flat text file on the system's hard drive. When you have only a few servers and services to check it's not so bad, but when you the more you add, the more IOPS you'll see. At 3,000 servers disk IO is a huge bottleneck.</p> <p>A lot of systems will be fairly responsive but show a really high load average, this is because of IO wait. Fortunately, the guy who setup Nagios at our DC was smart enough to realize we had a massive issue with disk IO and so he had everything running off 4x 15k RPM SCSI drives with a hardware RAID 10. Unfortunately, even with the fairly substantial hardware Nagios still took nearly 20 minutes to check every system in the datacenter. For a while, this was considered acceptable, because we didn't want to pay thousands of dollars for a commercial system and this particular admin was convinced that Nagios was running as fast as possible for now and that maybe the Nagios developers would speed things up in a later version.</p> <p>The old Nagios system grabbed it's information about what to monitor from a program we had called \"Server Locator\", which would soon be replaced with a database in Microsoft's SharePoint. So it fell to me to modify our existing Nagios system to grab it's configuration information from SharePoint instead of Server Locator. I had just recently been promoted to Jr. Admin, so I hadn't had a chance to look at Nagios and see how things were setup. I took one look at the system and decided it would be easier to setup a new one, on my terms. This meant that I could do things my way (hopefully that means the right way), and the old system could be kept ready in case the swichover didn't go smoothly.</p> <p>The first thing I looked into using was a distributed Nagios setup, but after only a day of playing with it I ran into a huge problem. It was slow, really slow, and I had no idea why. I had 3 boxes setup, the main system was called mother.nag, the others were named after the phases (sections of the datacenter) that they would be monitoring. Eventually I discovered that the problem was due to how Nagios communicated back to the main server. A daughter server (mother/dauther, get it? I know I'm clever) would perform a service check, and then report the result back to the mother server. Even if the check itself took only a fraction of a second, the entire exchange would always take at least 1 second. While this alone wasn't a huge issue, it was some fairly signifigant overhead. The real problem was that during the time that Nagios on the daughter server was communicating with the mother server it was doing nothing else. For some reason, during the communication it wouldn't do anything else, such as performing other service checks. This meant that if there were 300 servers in a phase (about average) it would take 5 minutes to scan the entire phase and report back.</p> <p>Maybe I'm just weird, but to me this was unacceptable, I knew Nagios could do things faster. The servers showed almost no load, or network traffic during normal operation, there was no reason why one system shouldn't be able to monitor everything in the datacenter. So I went back to the drawing board. Then it hit me, the system we had before only really had issues with disk IO, so what if I could just cut out that bottleneck, how fast would things go? So I moved the all the files nagios uses to a ramdisk, and then setup a quick cron-job to save them to disk once a minute.</p> <pre><code># m h  dom mon dow   command\n*/5 *   *   *   *    /usr/bin/rsync -a /dev/shm/ /var/nagios3/; rm /var/nagios3/lib/spool/checkresults/*; rm /var/nagios3/cache/nagios.tmp*; rm -rf /var/nagios3/nagios-config-*; chgrp www-data /dev/shm/lib/rw/nagios.cmd;\n</code></pre> <p>The server I had setup was using Ubuntu, which means it has a ramdisk mounted at /dev/shm. The ramdisk is allowed to use up to half of the memory in the system, which was fine with me since I had 2GB of ram and in total all the configuration files and cache files for Nagios came out to a whopping 16MB.</p> <p>With Nagios now using the ramdisk I decided to test it out by monitoring the everything in the datacenter. Why start small, if I was right this would be really fast, and sometimes I just love being recklessly over-confident in my ideas. So I started up Nagios and watched it make a scan of the datacenter. Once it was done I was disappointed, it was still taking too long, it took nearly 5 minutes to complete the scan of the datacenter. I was disappointed because even though I was now showing a 300% increase in speed over the original Nagios setup I noticed that my test box still have virtually no load while running the service checks. I knew I could make it go faster, but how would I do it?</p> <p>So I decided to throughly read the parts of the Nagios manual that I had glossed over, namely the section that deals with scheduling of host/service checks. I figured the issue was that the system just wasn't scheduling enough checks to happen in parrallel. This was backed up by the performance data, which showed that while my Check Execution Time was low (around 0.105 seconds on average) my Check Latency was much higher (I believe around 10 seconds or so on average). After much testing and tweaking I finally found the perfect settings. Please note, that the system I am running Nagios on has two dual-core Xeon processors. This is important to keep in mind, because the settings below will cause Nagios to spawn hundreds of processes at the same time. I strongly recommend you get as many cores (real cores, not hyper-threading) in your monitoring system so that you can make the most out of this setup.</p> <pre><code>service_inter_check_delay_method=0.01\nservice_interleave_factor=s\nhost_inter_check_delay_method=0.02\nmax_concurrent_checks=0\nuse_large_installation_tweaks=1\n</code></pre> <p>I set the Service Inter-Check Delay to a static 0.01 seconds, so that checks would happen as fast as possible. Combined with the Max Concurrent Checks set unlimited, this means that Nagios spawns processes for service checks like it's Zerg rushing. Luckily, the system's 4 cores handle everything pretty well. We only monitor one or two services (by default) for each server, so I didn't care too much about service interleaving, I left it up to Nagios to determine how to interleave service checks. Using Large Installation Tweaks is pretty standard for large Nagios installs, so you should already have that set. With these settings, and everything in the /dev/shm ramdisk, Nagios can now monitor every system in our datacenter in about 50 seconds. Yes, that's right, we can monitor the entire datacenter once a minute.</p> <p> </p> This screen lists out servers that are down (white means it's down but acknowledged) <p>I decided that in order to not overload customer's servers with traffic, that by default we'll run a service check every 3 minutes. If the a problem is detected, the interval for that service/host drops down to 1 minute until the service/host is determined to be in a \"hard down\" state. The best part is that because the old Nagios was so slow, we purchased a commercial system (Site Scope) to monitor the core infrastructure. About a week after my Nagios seutp went live it detected that the webserver that ran our main website went down, and so it sent out alerts to our blackberries. The commercial solution alerted us 4 minutes later. I was told that our CEO was extremely impressed with how quickly Nagios was able to detect and report the issue.</p> <p> </p> Good job, here is a lolcat <p>If you have any questions, feel free to use the comments section.</p> <p></p>","tags":["linux","nagios","ubuntu","work"]},{"location":"blog/2009/07/06/windows-7-the-top-5-things-i-like/","title":"Windows 7: The Top 5 Things I like","text":"<p>I have been running Windows 7 as my primary and only OS on my desktop and laptop for several months. I have no plans of going back to XP. This OS is rock-solid stable and fast. Great hardware support and new features and enhancements on old features make it by far the most attractive OS on the horizon, as far as I am concerned. I am, without a doubt, planning to purchase Windows 7 Ultimate when it releases, too bad it is not available for half off preorder. Here are my own top 5 reasons why Windows 7 is awesome.</p>","tags":["microsoft","toplist","windows-7"]},{"location":"blog/2009/07/06/windows-7-the-top-5-things-i-like/#no5-the-task-bar","title":"No5. The Task Bar","text":"<p>Everyone and their dog have written about how nice and neat the new task bar is. I am thoroughly pleased with the changes that Microsoft has made to the task bar. I like the large hit area for launching and switching applications, the color changing behind the icon, the live Aero Peek thumbnail and tabbed Aero Peek support, the download or status background, the tabs it uses to show multiple applications. Launching and switching applications is so easy because of the large hit areas and the applications are easier than ever to distinguish with their extra large icons. The color change behind the icons when you mouse over is a nice aesthetic touch, if nothing else, and brings a level of depth to the buttons. I like the live thumbnail preview provided by Aero Peel and the previews are quite a bit larger than before and are click able. You can use them to close specific instances of an application or, of course, bring up a certain program. While I do genuinely like the new task bar, it has some shortcomings, but that is another post.</p>","tags":["microsoft","toplist","windows-7"]},{"location":"blog/2009/07/06/windows-7-the-top-5-things-i-like/#no4-resource-monitor","title":"No4. Resource Monitor","text":"<p>Any power user will undoubtedly see the potential in Resource Monitor. It is a cross between Sysinternal's Process Explorer, TCPView and Filemon. Not sure what is tanking your CPU? Open up Task manager and see, but if you need any more information than that, you want Resource Monitor. Want to know where your Network Bandwidth is going? Again, Resource Monitor is there for you. Curious about your Disk I/O or what is writing to the disk, again Resource Monitor. The more I use it the more useful I find it. You can drill down on applications and find out about the threads and each handle and service associated with it. Oh, also, thank you Windows Vista for this feature.</p>","tags":["microsoft","toplist","windows-7"]},{"location":"blog/2009/07/06/windows-7-the-top-5-things-i-like/#no3-the-hardware-support","title":"No3. The Hardware Support","text":"<p>Many critics claim that Windows Vista has very poor hardware support. In my experience, I have found that to be somewhat true. For the most part, it is not Microsoft's fault, but they do take most of the flack for it. I was working retail computer sales when Windows Vista hit shelves. I remember quite a few people coming to the store only a few weeks after Vista's release complaining that X, Y and Z did not work on their new computer.\u00a0 At first, I thought it would be a simple issue of directing the customer to the Manufacture's website to download the latest drivers, but I soon found this was not going to work. Many, many manufactures seemed to relish the opportunity to force customers to upgrade to the Windows Vista version of their product.</p> <p>I have found that hardware support is superb with Windows 7. After the initial install of Windows 7 Windows Update automatically detected every single piece of hardware in the computer, except one, downloaded and installed the proper driver. It detected my Nvidia graphics card, my Dlink Wireless card, my motherboard chipset. Everyone I know that has installed Windows 7 has had the exact same results. The ONLY piece of hardware it did not install drivers for was my Creative sound card. I found the driver on Creative's website. While it did detect the driver, the graphics card driver was not up to date, no problem I just installed the Windows Vista x64 driver. Two years after the release of Vista it seems every component made supports Vista x86 and Vista X64 and those drivers, in my own experience, work just fine with Windows 7.</p>","tags":["microsoft","toplist","windows-7"]},{"location":"blog/2009/07/06/windows-7-the-top-5-things-i-like/#no2-the-speed","title":"No2. The Speed","text":"<p>Windows 7 is FAST. In comparison to Windows Vista, well actually I cannot make that comparison. I have never run 7 and Vista on the same hardware. I built a new system and the only OS that has been on it is 7 beta and 7 RC. At first, I thought it was the new box, but then I installed 7 on my laptop. My laptop is 3 years old now, a core duo 1.66GHz with 2 GB of ram. Windows 7 is faster than XP as far as boot and shut down are concerned. Applications seem to launch faster, but I must admit, this is ALL subjective. I never timed anything before or after I installed Windows 7. To further complicate the issues a fresh install over will always seem faster than a bloated install, my XP install I blew away was by no definition of the word lean.\u00a0 It could be a complete case of confirmation bias, or worse yet, all in my head, but it really does feel faster. After reading quite a few articles online, I feel somewhat vindicated in my belief that the OS is, in fact, faster than Vista and potentially faster than XP.</p>","tags":["microsoft","toplist","windows-7"]},{"location":"blog/2009/07/06/windows-7-the-top-5-things-i-like/#no1-its-not-vista","title":"No1. It's not Vista","text":"<p>Macintosh has successfully demonized Vista so well with their \"I'm a Mac, I'm a PC\" commercials, more power to them, that no one will touch it with a ten-foot pole. I am not going to go into the shortcomings of Vista beyond the atrociously poor PR. Vista's biggest down fall has the potential to be Windows 7's greatest achievement -- it is not Windows Vista! Competition breeds excellence and the heat Mac has put on Microsoft lately has really put Microsoft in the mood to play hardball and it shows in Windows 7. Let us just hope people understand Windows 7 is not just a rehash of Windows Vista with all the same, \"problems.\"</p> <p>Of course, like always, if you have any feedback or comments I welcome them.</p>","tags":["microsoft","toplist","windows-7"]},{"location":"blog/2010/08/19/compiling-bitcoin-on-ubuntu-1004-lucid/","title":"Compiling Bitcoin on Ubuntu 10.04 (Lucid)","text":"<p>In case you don't know, Bitcoin is a decentralized crypto-currency or as the site says a \"peer-to-peer network based digital currency\". While most people have been content to use the pre-compiled binaries available, some of us prefer to compile the client from source, for various reasons. Personally, I maintain a repository on github because I like using a number of patches that various programmers have put forth in the Bitcoin community. Whatever your reason for compiling from source, you have likely run into issues with compiling the graphical client since it requires the development version of wxWidgets. After a whole lot of failed attempts to compile the Bitcoin client I finally found a working configuration.</p> <p>I have tested this process only on Ubuntu 10.04 (Lucid), both 64 and 32bit. From what I can tell earlier versions of Ubuntu and Debian should work just fine, but with a different set of packages. Specifically the \"libboost-all-dev\" package appears to only be available in Ubuntu 10.04, but the \"libboost-dev\" package may work for earlier versions and/or Debian.</p> <p>First you need to install the proper packages:</p> <pre><code>sudo apt-get install build-essential libgtk2.0-dev libssl-dev libdb4.7-dev libdb4.7++-dev libboost-all-dev checkinstall subversion git-core\n</code></pre> <p>Next, you will need to download the 2.9 version of wxWidgets from their SourceForge project. I suggest creating a folder in your home directory called \"src\", where we can compile wxWidgets and Bitcoin. Once the folder has been created we can download the 2.9 version of wxWidgets from their SourceForge project, and extract it. I've written out example steps below:</p> <pre><code>cd ~\nmkdir src\ncd src\nwget \"http://downloads.sourceforge.net/project/wxwindows/wxAll/2.9.0/wxWidgets-2.9.0.tar.gz?r=&amp;ts=1282200132&amp;mirror=surfnet\"\ntar -zxvf wxWidgets-2.9.0.tar.gz\ncd wxWidgets-2.9.0/\n</code></pre> <p>Now that wxWidgets is extracted, you are ready to begin compiling it. First, create a build directory and change to it.</p> <pre><code>mkdir buildgtk\ncd buildgtk\n</code></pre> <p>Now configure and build wxWidgets:</p> <pre><code>../configure --with-gtk --enable-debug --disable-shared --enable-monolithic\nmake\n</code></pre> <p>We could just run \"sudo make install\", but that would put a bunch of wxWidget files all throughout your filesystem with no way to easily remove them. Luckily, the checkinstall program allows you to create a Debian package on the fly! This allows you to quickly and easily remove the files installed by wxWidgets. Let's get started:</p> <pre><code>sudo checkinstall\n</code></pre> <p>You'll probably be prompted to \"create a default set of package docs\", just go ahead and hit enter. When it prompts you for the package description, just put \"wxwidgets\" without the quotes and hit enter twice. You should have a screen that looks like this:</p> <pre><code>*****************************************\n**** Debian package creation selected ***\n*****************************************\nThis package will be built according to these values:\n0 -  Maintainer: [ root@ubuntu ]\n1 -  Summary: [ wxwidgets ]\n2 -  Name:    [ build ]\n3 -  Version: [  ]\n4 -  Release: [ 1 ]\n5 -  License: [ GPL ]\n6 -  Group:   [ checkinstall ]\n7 -  Architecture: [ i386 ]\n8 -  Source location: [ build ]\n9 -  Alternate source location: [  ]\n10 - Requires: [  ]\n11 - Provides: [ build ]\nEnter a number to change any of them or press ENTER to continue:\n</code></pre> <p>Select \"3\" for version, then put \"2.9\" as the\u00a0version number. Once you are back at the package creation menu, hit enter again to install wxWidgets and build the package. Once it's done, you will need to run the following command:</p> <pre><code>sudo ldconfig\n</code></pre> <p>Now you are ready to download the Bitcoin source and compile the client! Change back to the \"src\" directory we created earlier:</p> <pre><code>cd ~/src\n</code></pre> <p>If you want to download the latest version of Bitcoin from the official subversion repository, run the following:</p> <pre><code>svn co https://bitcoin.svn.sourceforge.net/svnroot/bitcoin/trunk bitcoin-trunk\n</code></pre> <p>Optionally you can download the source code for a different version of the Bitcoin client, such as from my git repository (with various community patches I've added):</p> <pre><code>git clone\u00a0git://github.com/aceat64/bitcoin-patchwork.git\n</code></pre> <p>Once the source code has been downloaded you will need to change to the directory of whichever version you picked. Once there you only need to run one command to compile the Bitcoin graphical client:</p> <pre><code>make -f makefile.unix bitcoin\n</code></pre> <p>If you want to compile the bitcoin daemon, the command is:</p> <pre><code>make -f makefile.unix bitcoind\n</code></pre> <p>If everything went well there will be a file called \"bitcoin\" in the current directory. I suggest copying the file to \"/usr/local/bin\" and making sure it's owned as root with the permissions 755 so that no one but root can modify it.</p> <p>If this article helped you out, please send some Bitcoins to the Bitcoin Faucet and help spread the word about Bitcoin.</p>","tags":["bitcoin","cryptography","linux","sourceforge","ubuntu"]},{"location":"blog/2010/11/17/dallas-makerspace-open-house/","title":"Dallas Makerspace Open House","text":"<p>A while back I became a member of the Dallas Makerspace, which is basically the only hackerspace in the Dallas area. I met a lot of cool people and now I have access to a lot of really cool tools (Makerbot, CNC laser cutter, etc) so hopefully I'll have some new blog posts in the next few months talking about a cool project I'm working on. If you live in the Dallas area or will be here on the 20<sup>th</sup>, come by our open house, it's going to be fun.</p> <p>From the Dallas Makerspace blog:</p> <p>On November 20<sup>th</sup>,\u00a0DMS will be hosting an\u00a0Open House for all who want to come and check out just what a Makerspace is all about. There will be food, demonstrations, tours, and a meet and greet with the Dallas Makerspace members. Meet people passionate about a myriad of things from robotics, to wood crafting, to art and photography.</p> <p>Come! And see the wonders of laser cutters, Cupcake CNC Makerbots, and\u00a0darkroom science! See demos about brass etching,\u00a0recycling plastic bags into sewable fabric and view\u00a0projects DMS members are currently working on: Eggbot, the\u00a0Jacob\u2019s Ladder and more!</p> <p>Come! And learn about what a Makerspace is; our goals, projects, and purpose.</p> <p>Come! And enjoy a day full of scientific endeavors, artistic pursuits, and innovative thinking!</p> <p>When: Saturday, November 20, 2010, from Noon until Midnight Where: Dallas Makerspace, 11020 Audelia Rd, Suite C103, Dallas, TX 75243 Note: Make sure to check out the\u00a0wiki for a full list of demos, projects and links to a map that will lead you to our doorstep.</p> <p>View\u00a0Secret Path to DMS in a larger map</p>","tags":["dallas-area","dms","hackerspace","makerspace"]},{"location":"blog/2011/07/16/conversation-with-a-spam-bot/","title":"Conversation with a Spam Bot","text":"<p>Today, while I was working on the inventory system\u00a0for the Dallas Makerspace\u00a0I\u00a0received\u00a0a message from an unknown person via AIM. This is not an unusual event for me since I have had the same AIM screen name for over a decade, so I tend to get messages out of the blue from people I used to know or who I gave my info to and promptly forgot (sorry). \u00a0While this wasn't the first spam bot to contact me, it was the first to fool me into thinking it was a real human, at least for a bit. Here's the chat log, with my comments:</p> <p>bluesoftball27: heya! me: Hello, do I know you? bluesoftball27: hey whatss up? 24/f here u? me: Who are you? bluesoftball27: Im natali. Dont you remember? me: I'm sorry, I don't. Did we meet somewhere? bluesoftball27: oh cool.. have we talked b4? me: Not that I recall bluesoftball27: ah ok sorry. wasnt sure. but anywho whats up? me: I'm writing an inventory management program at the moment bluesoftball27: yeah? im so bored over here i dunno what to do with myself. bluesoftball27: you wanna watch me on cam? i feel like having some fun :-X</p> <p>Here is where I started suspecting that bluesoftball27 might be up to something.</p> <p>me: I'll pass, I'm kind of busy bluesoftball27: k u gotta make a free login for this site that its linked to so i cant be recorded. k?</p> <p>Yup, this is definately someone trying to trick me into downloading a virus or giving out my credit card info. However, at this point I had not yet realized the responses were automated.</p> <p>me: No thanks, I don't need a virus right now. bluesoftball27: k cool. http://[redacted]/7/natali go there and when it loads click JOlN FREE at the top ok? me: Ahhh, I see. These are all preprogrammed responses.</p> <p>Finally I realized the horrible, awful, truth. Natali was a program, sent back in time to steal my credit card, or something.</p> <p>bluesoftball27: it does ask for a credit card i think but i know for sure it dosent cost anything bluesoftball27: its just to make sure that you are over 18. me: Empty response bluesoftball27: when u get signed up then u can come watch me on cam and we can have some fun. me: What other responses do you have? me: Are they static, or based on my input? bluesoftball27: if u join me in private or tip me some gold i will do anything u ask me babe bluesoftball27: i really mean ANYTHlNG. :-X me: Anything? But you're not even real bluesoftball27: lets talk on the site when u get in babe. my aim is messing up me: Your aim is messing up? What is this, 1998? me: Hello?</p>","tags":["bot","funny","phishing","spam","virus"]},{"location":"blog/2011/12/06/mail-checker-minus-or-why-are-you-maintaining-a-chrome-extension/","title":"Mail Checker Minus; Or Why Are You Maintaining a Chrome Extension?","text":"<p>Outdated</p> <p>I am no longer maintaining the Mail Checker Minus extension. I recommend switching over to Jason Savard's Checker Plus For Gmail. He's already done a good bit of work cleaning up code and adding new features. For those of you happy with the extension as is, I will continue to make it available via the Chrome Web Store, but please note that I will not be fixing bugs or supporting the extension.</p> <p>It's fairly likely that you found this site because of the Google Chrome Extension I now maintain, Mail Checker Minus. That's a pretty good assumption since it's fairly popular and links here. For the two people who somehow found my site and don't know about Mail Checker Minus, it is (in my opinion) one of the most useful plugins for Chrome. It allows you to see the number of unread mails for Gmail at a glance, preview them, and even take actions on them.</p> <p></p> <p>I'm not tooting my own horn here, I didn't write Mail Checker Minus, I forked it from an extension called \"Mail Checker Plus for Google Mail\". I loved Mail Checker Plus but sometime earlier this year the developer decided to modify the extension so that it injected ads into certain websites. Obviously that is a pretty awful thing to do, from what I can tell Google quickly removed Mail Checker Plus from the Chrome Web Store.</p> <p>I first noticed that the extension was gone when I installed Chrome on a new computer and the only extension that didn't auto install (sync everything is AWESOME) was Mail Checker Plus. After a bit of searching I discovered that my favorite extension was dead and gone, but the source code for it was on Github and released under the GPL. Before I even had a chance to think about it I had clicked \"fork\" on Github. I cloned the repo to my local computer, installed the plugin and promptly went back to my normal work.</p> <p>A few days later I decided, mostly to make my life easier (I really like being able to sync everything), to rename the extension to Mail Checker Minus and put in the Chrome Web Store. I figured no one would ever see it, but I didn't really care so long as the extension kept working for me. As it turns out there were quite a few people who installed the new extension, as of this writing there are 3,677 users and the extension is rated 5 stars. I think that is pretty awesome, so for now I guess I will keep on maintaining the Mail Checker Minus extension. I would like to thank everyone who has or will help my favorite extension continue to grow and survive.</p> <p>P.S. If you think you can make a better extension, DO IT, please.</p>","tags":["github","gmail","google","foss"]},{"location":"blog/2013/01/03/smart-guns-dangerously-dumb/","title":"\"Smart\" Guns - Dangerously Dumb","text":"<p>Recently on NerdAbsurd\u00a0we discussed \"Smart\" guns, in which I took up the task of refute an article on\u00a0Slate.com\u00a0written by\u00a0Farhad Manjoo. When\u00a0Virginia\u00a0first sent me a link to the article asking my opinion, I knew I would have more than a few things to say after reading just the title: \"We Have the Technology To Make Safer Guns: Too bad gunmakers don\u2019t care.\"\u00a0For now, I'm going to overlook the idea that there is some grand conspiracy of firearms manufacturers and tackle the idea of \"smart\" guns themselves.</p>","tags":["firearms","politics","safety"]},{"location":"blog/2013/01/03/smart-guns-dangerously-dumb/#technology-is-not-a-fix-for-poor-safety-practices","title":"Technology is not a fix for poor safety practices","text":"<p>I do not believe in the idea of \"Accidental\" Discharges (AD's in gun-lingo), and instead subscribe to the idea of Negligent Discharges (ND's). To me, anytime a firearm is fired\u00a0unintentionally\u00a0it is not an accident, it is negligence on the part of the operator. A modern firearm does't just \"go off\", they have various safeties to ensure that a round is only fired when someone pulls the trigger. For example, I own a Springfield Armory XD-9, it has the following safety features:</p> <ul> <li>Grip safety - Unless the grip safety is depressed, the trigger can not be pulled and the slide can not be actuated.</li> <li>Trigger safety - The trigger is made of two pieces, requiring the user to press in on the center of the trigger, and limiting the ability for foreign objects to \"grab\" on to the trigger and actuate it.</li> <li>Drop safety - This part is a small block that prevents the\u00a0striker\u00a0from hitting the firing pin. It is pushed out of the way only when the grip safety is depressed and the trigger is pulled. In it's default state, the drop safety prevents a round from being fired if the firearm is dropped or hit.</li> </ul> <p>Some of these are not new features, the extremely popular 1911, which as been around since, well 1911, has a grip safety.</p> <p>So if we've ruled out the possibility that guns just \"go off\", what is the root cause of these accidents? Negligence and poor safety practices. As with any dangerous piece of equipment (cars, table saws, fire, etc) there are safety rules that should be followed. For firearms, some of the most basic rules are:</p> <p></p> <ol> <li>Treat every gun as if it is loaded.</li> <li>Don't point a gun at something unless you intend to shoot it.</li> <li>Don't put your finger on the trigger unless you intend to fire.</li> </ol> <p>Virtually all Negligent Discharges are caused by the operator failing to follow one of those rules. A fairly common one is pulling the trigger without thinking, when grasping a firearm while not paying attention, it is easy for your finger to slide into the trigger guard and pull the trigger when you tighten your group.</p> <p>As this image\u00a0demonstrates\u00a0 you should get in the habit of resting your index finger alongside the gun, NOT on the trigger.</p> <p>Mainly, this is an issue of training. We teach (or should) safe sex in schools, it shouldn't be controversial to also teach people how to safely handle firearms. In my opinion, you put your child at risk if you don't teach them firearm safety, because their only knowledge would be from movies, TV shows and video games, which more often than not, portray poor safety practices or outright nonsense with regards to firearms.</p>","tags":["firearms","politics","safety"]},{"location":"blog/2013/01/03/smart-guns-dangerously-dumb/#1-in-100-is-a-terrible-failure-rate-for-a-modern-firearm","title":"1 in 100 is a terrible failure rate for a modern firearm","text":"<p>Many people own firearms so that they can protect themselves or their loved ones. Part of what makes a modern firearm a great defensive weapon is reliability.\u00a0Here is what Farhad wrote in the article on the system's reliability, or rate of successful identification:</p> <p>\u201cWe\u2019re better than a 99 percent false negative rate,\u201d Sebastian says. \u201cAn authorized user will be recognized 99 times out of a 100.\u201d (This does mean that one time out of 100, you\u2019ll go to fire your gun and it won\u2019t shoot\u2014though as Sebastian points out, guns often fail for mechanical reasons, so a 1 percent chance of failure wasn\u2019t considered a fatal flaw.)</p> <p>I have fired around 5,000 rounds through my XD-9, with only 1 failure (ammo related). This is not a statistical\u00a0anomaly, modern firearms, using good ammo, are\u00a0incredibly\u00a0reliable.\u00a0I wouldn't trust the defense of my family to a firearm that had a 1 in 100 chance of failing on each trigger pull. Even if the technology for these \"smart\" guns had a failure rate of 1 in 1000, the technology is still more unreliable then your average pistol.</p> <p>Here's the real kicker, according to the paper \"Handgrip Recognition\"\u00a0that Farhad linked to in his article, the\u00a0actual\u00a0rate of successful identification was only\u00a089.44%, roughly a 1 in 10 chance that the system will fail to properly\u00a0recognize\u00a0the user. Many pistols have 10, 13 or 16 round magazines. If I owned a firearm that failed roughly once per magazine, I would send it in for repair.</p>","tags":["firearms","politics","safety"]},{"location":"blog/2013/01/03/smart-guns-dangerously-dumb/#smart-guns-allow-people-to-wrongly-feel-complacent-with-safety","title":"Smart guns allow people to (wrongly) feel complacent with safety","text":"<p>We have already identified that accidents happen because people neglect safety, what happens if we throw a \"smart\" gun into the mix? It is easy for people to get complacent, but to do so with safety (for any tool) is a\u00a0recipe\u00a0for disaster. \"Smart\" guns allow people to wrongly feel that they are safe handling someone else's weapon, because it \"can't\" be fired by unauthorized users.\u00a0If this technology is ever mandated, I imagine that it would not be too long before we see accidents where people shoot themselves or their friends because \"it\u00a0shouldn't have fired\".</p> <p>One thing I found odd was that the paper made no mention of the false positive rate, which is how often the system would incorrectly identify an unauthorized user as authorized, allowing them to fire the weapon. Let's be generous and pretend the rate is 1 in 100. People are curious, they'll want to see if the biometrics really work, so obviously they'll try it out, likely with an unloaded gun. Most of the time, it will probably work just fine, but now the person has a false belief that the system works, it's \"safe\", they easily grow complacent. What happens later on, when they're showing off the system to friends, and that 1 in 100 false positive strikes? Potentially, someone gets shot by a gun that \"shouldn't have fired, it had a safety!\"</p> <p>You can't fix poor safety practices with technology, you can only fix them with education and\u00a0vigilance.</p>","tags":["firearms","politics","safety"]},{"location":"blog/2013/01/03/smart-guns-dangerously-dumb/#smart-guns-are-a-solution-in-search-of-a-problem","title":"Smart guns are a solution in search of a problem","text":"<p>After hearing about some of the heartbreaking incidents such as\u00a0Joseph Loughrey shooting his 7 year old son, you might be wondering, how common are these\u00a0tragedies?\u00a0Fortunately, according to the CDC they are exceedingly rare, roughly 600/year. While each life lost is tragic, when making policy\u00a0decisions\u00a0we have to look at real world risks. Firearm accidents comprise only 0.5% of all fatal accidents, they are far outweighed by deaths from\u00a0motor vehicle traffic (34%), poisoning(24.1%), falls(18.3%), suffocation(4.8%), drowning(2.8%), and fire/burns(2.7%).</p> <p>We don't have to look to the latest James Bond style gadget to make our lives safer, plenty of guns already come with good, solid safety features. The missing component for many people, is knowledge. If you own firearms, not only do you need to be aware of how to safely handle firearms, it is up to you to teach those around you how to properly handle these tools. Everyone should be taught the basics of firearm safety, you should teach your children, family and friends, even if they don't own guns.</p> <p>For more information on firearm safety, please check out these links:</p> <ul> <li>http://darkwing.uoregon.edu/~joe/firearms-safety.html</li> <li>http://nssf.org/safety/basics/</li> <li>http://eddieeagle.nra.org/</li> </ul>","tags":["firearms","politics","safety"]},{"location":"blog/2016/04/12/compiling-open-zwave-control-panel-on-a-raspberry-pi-3/","title":"Compiling open-zwave-control-panel on a Raspberry Pi 3","text":"<p>I've recently started diving into home automation stuff, and of course immediately ran into issues. I purchased a z-wave water sensor and planned to use Home Assistant to monitor the sensor and send me alerts (via pushover) if it detects water. While getting HA installed was quick and easy, the trouble began when I tried to actually read from the sensor. I could see in the OZW_Log.txt log file that the system was receiving data, but the HA log had no event information.</p> <p>After a bunch of searching, I came across a post talking about using open-zwave-control-panel (ozwcp) to diagnose and configure z-wave devices. Awesome! Sadly, the documentation for this project is lacking, and it took a bit of trial and error to figure out the right steps to get ozwcp build, let alone run. Eventually I figured it out, reconfigured my sensor and everything worked. So to save others the frustration of trying to build ozwcp, I've got a quick and simple tutorial for you!</p> <p>Update</p> <p>There's a docker image that's only slightly out of date, I suggest trying that first\u00a0before going to the effort of compiling ozwcp. If you don't have docker installed, follow one of the many tutorials out there, then run this command (where /dev/ttyUSB0 is your zwave controller):</p> <pre><code>docker run -p 8008:8008 --device /dev/ttyUSB0 openzwave/openzwave-control-panel &amp;&gt;/dev/null &amp;\n</code></pre> <p>And now back to our regularly scheduled programming.</p> <p>Please note, these instructions have only been tested on\u00a0Raspbian 8 (jessie) on a Raspberry Pi 3. Your milage may vary! All commands are run as\u00a0root, because I'm lazy.</p> <p>First, install some (probably) important libraries:</p> <pre><code>apt-get update\napt-get install libgnutls28-dev libgnutlsxx28\n</code></pre> <p>Download, build and install libmicrohttpd</p> <pre><code>cd\nwget ftp://ftp.gnu.org/gnu/libmicrohttpd/libmicrohttpd-0.9.19.tar.gz\ntar zxvf libmicrohttpd-0.9.19.tar.gz\nmv libmicrohttpd-0.9.19 libmicrohttpd\ncd libmicrohttpd\n./configure\nmake\nmake install\n</code></pre> <p>Download and build the open-zwave library</p> <pre><code>cd\ngit clone https://github.com/OpenZWave/open-zwave.git\ncd open-zwave\nmake\n</code></pre> <p>Download open-zwave-control-panel</p> <pre><code>cd\ngit clone https://github.com/OpenZWave/open-zwave-control-panel.git\ncd open-zwave-control-panel\n</code></pre> <p>Open the\u00a0Makefile and find the following line:</p> <pre><code>OPENZWAVE := ../\nLIBMICROHTTPD := -lmicrohttpd\n</code></pre> <p>Change it to:</p> <pre><code>OPENZWAVE := ../open-zwave\nLIBMICROHTTPD := /usr/local/lib/libmicrohttpd.a\n</code></pre> <p>Build\u00a0ozwcp</p> <pre><code>make\nln -sd ../open-zwave/config\n</code></pre> <p>And finally, run it!</p> <pre><code>./ozwcp -p 8888\n</code></pre> <p>Now you can open <code>http://your\\_system:8888</code> in a browser to start using the open-zwave-control-panel!</p>","tags":["automation","linux","raspberry-pi","zwave"]},{"location":"guides/macos-setup/","title":"macOS Setup","text":"<p>Originally I wrote this guide to remind myself how to setup my M1 Macbook the way I like it. A few friends and coworkers expressed interest in my setup, so I cleaned up the notes (a bit) and created this guide. You don't need to follow this guide completely, I encourage you to pick-and-choose the pieces you want to use and to tweak things to fit your needs. I also love to see how other people have setup their systems, so please share your configs!</p>"},{"location":"guides/macos-setup/#recommended-software","title":"Recommended Software","text":"<p>Download and install these following the instructions on their websites:</p> <ul> <li>iTerm2</li> <li>Visual Studio Code</li> <li>Homebrew</li> </ul>"},{"location":"guides/macos-setup/#command-line-tools","title":"Command Line Tools","text":""},{"location":"guides/macos-setup/#btop","title":"Btop++","text":"<p>TUI resource monitoring. C++ version and continuation of bashtop and bpytop.</p> <pre><code>brew install btop\n</code></pre> <p>https://github.com/aristocratos/btop</p>"},{"location":"guides/macos-setup/#mosh","title":"Mosh","text":"<p>Better SSH for high latency or unreliable connections.</p> <pre><code>brew install mosh\n</code></pre> <p>https://mosh.org/</p>"},{"location":"guides/macos-setup/#mtr-traceroute","title":"mtr (traceroute)","text":"<p>TUI traceroute utility.</p> <pre><code>brew install mtr\n</code></pre> <p>https://github.com/traviscross/mtr</p>"},{"location":"guides/macos-setup/#nmap","title":"Nmap","text":"<p>Network scanner.</p> <pre><code>brew install nmap\n</code></pre> <p>https://nmap.org/</p>"},{"location":"guides/macos-setup/#jq","title":"jq","text":"<p>Command line JSON parser.</p> <pre><code>brew install jq\n</code></pre> <p>https://stedolan.github.io/jq/</p>"},{"location":"guides/macos-setup/#fq","title":"fq","text":"<p>Like JQ but for binary formats.</p> <pre><code>brew install wader/tap/fq\n</code></pre> <p>https://github.com/wader/fq</p>"},{"location":"guides/macos-setup/#bat","title":"bat","text":"<p>A cat(1) clone with syntax highlighting and Git integration.</p> <pre><code>brew install bat\nbrew install eth-p/software/bat-extras\n</code></pre> <p>While it's optional, I recommend installing the bat-extras, it's a collection of bash scripts that integrate <code>bat</code> with various command line tools.</p> <p>To easily format help output from a command, I put the following alias and function in my <code>~/.zshrc</code> file:</p> <pre><code># ~/.zshrc\nalias bathelp='bat --plain --language=help'\nhelp() {\n\"$@\" --help 2&gt;&amp;1 | bathelp\n}\n</code></pre> <p>https://github.com/sharkdp/bat</p>"},{"location":"guides/macos-setup/#fd","title":"fd","text":"<p>A simple, fast and user-friendly alternative to <code>find</code></p> <pre><code>brew install fd\n</code></pre> <p>https://github.com/sharkdp/fd</p>"},{"location":"guides/macos-setup/#developer-tools","title":"Developer Tools","text":""},{"location":"guides/macos-setup/#postgresql","title":"PostgreSQL","text":"<p>Database software.</p> <pre><code>brew install postgresql\n</code></pre> <p>https://www.postgresql.org/</p>"},{"location":"guides/macos-setup/#lefthook","title":"Lefthook","text":"<p>Git hooks manager, useful for development.</p> <pre><code>brew install lefthook\n</code></pre> <p>https://github.com/evilmartians/lefthook</p>"},{"location":"guides/macos-setup/#poetry","title":"Poetry","text":"<p>A Python dependency manager and packaging tool.</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>https://python-poetry.org/</p>"},{"location":"guides/macos-setup/#git-delta","title":"git-delta","text":"<p>A syntax-highlighting pager for git, diff, and grep output.</p> <pre><code>brew install git-delta\n</code></pre> <p>Update your <code>~/.gitconfig</code> file:</p> <pre><code># ~/.gitconfig\n[core]\npager = delta\n\n[interactive]\ndiffFilter = delta --color-only\n[add.interactive]\nuseBuiltin = false # required for git 2.37.0\n\n[delta]\nnavigate = true    # use n and N to move between diff sections\nlight = false      # set to true if you're in a terminal w/ a light background color (e.g. the default macOS terminal)\n\n[merge]\nconflictstyle = diff3\n\n[diff]\ncolorMoved = default\n</code></pre> <p>https://github.com/dandavison/delta</p>"},{"location":"guides/macos-setup/#prettier","title":"Prettier","text":"<p>Code formatter for JavaScript, CSS, JSON, GraphQL, Markdown, YAML.</p> <pre><code>brew install prettier\n</code></pre> <p>https://prettier.io/</p>"},{"location":"guides/macos-setup/#version-managers","title":"Version Managers","text":"<p>There are helpful tools to install and run multiple versions of various programming languages.</p>"},{"location":"guides/macos-setup/#python-pyenv","title":"Python (pyenv)","text":"<pre><code>brew install pyenv\n</code></pre> <p>https://github.com/pyenv/pyenv</p>"},{"location":"guides/macos-setup/#nodejs-nvm","title":"NodeJS (nvm)","text":"<p>Once the install completes, be sure to follow the instructions provided and add the required lines to your ~/.zhsrc file.</p> <pre><code>brew install nvm\n</code></pre> <p>https://github.com/nvm-sh/nvm</p>"},{"location":"guides/macos-setup/#terraform-tfenv","title":"Terraform (tfenv)","text":"<pre><code>brew install tfenv\n</code></pre> <p>https://github.com/tfutils/tfenv</p>"},{"location":"guides/macos-setup/#kubernetes-tools","title":"Kubernetes Tools","text":""},{"location":"guides/macos-setup/#kubectl","title":"kubectl","text":"<p>Basic Kubernetes command line tools, such as kubectl.</p> <pre><code>brew install kubernetes-cli\n</code></pre> <p>https://kubernetes.io/</p>"},{"location":"guides/macos-setup/#kubectx","title":"kubectx","text":"<p>Switch between Kubernetes contexts faster and easier.</p> <pre><code>brew install kubectx\n</code></pre> <p>https://github.com/ahmetb/kubectx</p>"},{"location":"guides/macos-setup/#helm","title":"Helm","text":"<p>Kubernetes package manager.</p> <pre><code>brew install helm\n</code></pre> <p>https://helm.sh/</p>"},{"location":"guides/macos-setup/#k9s","title":"k9s","text":"<p>TUI for Kubernetes.</p> <pre><code>brew install derailed/k9s/k9s\n</code></pre> <p>https://k9scli.io/</p>"},{"location":"guides/macos-setup/#zsh-setup","title":"ZSH Setup","text":""},{"location":"guides/macos-setup/#features","title":"Features","text":"<ul> <li>Oh My ZSH</li> <li>zsh-autosuggestions</li> <li>terminal-notifier</li> <li>autoupdate</li> <li>fast-syntax-highlighting</li> <li>Powerlevel10k Theme</li> </ul>"},{"location":"guides/macos-setup/#step-by-step-instructions","title":"Step-By-Step Instructions","text":"<ol> <li> <p>Install Oh My ZSH:</p> <pre><code>sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> </li> <li> <p>Restart iTerm</p> </li> <li> <p>Install powerlevel10k:</p> <pre><code>git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\n</code></pre> </li> <li> <p>Edit <code>~/.zshrc</code>, set <code>ZSH_THEME=\"powerlevel10k/powerlevel10k\"</code></p> </li> <li>Restart iTerm</li> <li>Follow setup directions from p10k</li> <li> <p>Install terminal-notifier, required for the <code>bgnotify</code> Oh My ZSH plugin:</p> <pre><code>brew install terminal-notifier\n</code></pre> </li> <li> <p>Install zsh-autosuggestions:</p> <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\n</code></pre> </li> <li> <p>Install autoupdate:</p> <pre><code>git clone https://github.com/TamCore/autoupdate-oh-my-zsh-plugins $ZSH_CUSTOM/plugins/autoupdate\n</code></pre> </li> <li> <p>Install fast-syntax-highlighting:</p> <pre><code>git clone https://github.com/zdharma-continuum/fast-syntax-highlighting.git \\\n${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/plugins/fast-syntax-highlighting\n</code></pre> </li> <li> <p>Edit <code>~/.zshrc</code></p> <ol> <li> <p>Add the following lines to the end of the file:</p> <pre><code>ZSH_AUTOSUGGEST_STRATEGY=(history completion)\nZSH_AUTOSUGGEST_BUFFER_MAX_SIZE=20\nbgnotify_threshold=30\n</code></pre> </li> <li> <p>Edit the <code>plugins</code> setting:</p> <pre><code>plugins=(\nautoupdate\n    aws\n    bgnotify\n    fast-syntax-highlighting\n    macos\n    terraform\n    safe-paste\n    zsh-autosuggestions\n)\n</code></pre> </li> </ol> </li> <li> <p>Install fzf for better history searching (ctrl-r):</p> <pre><code>brew install fzf\n\n# To install useful key bindings and fuzzy completion:\n$(brew --prefix)/opt/fzf/install\n</code></pre> </li> </ol>"},{"location":"guides/macos-setup/#optional-vs-code-settings","title":"Optional: VS Code Settings","text":"<p>VS Code users will need to add the following to their <code>settings.json</code>:</p> <pre><code>\"terminal.external.osxExec\": \"iTerm.app\",\n\"terminal.integrated.defaultProfile.osx\": \"zsh\",\n\"terminal.integrated.fontFamily\": \"MesloLGS NF\"\n</code></pre>"},{"location":"guides/mkdocs-material-insider-github-pages-action/","title":"Deploy Material for MkDocs Insiders Using GitHub Actions","text":"<p>This site is built using Material for MkDocs Insiders, the sponsorware version of Material for MkDocs, which includes a number of features not in the publically available version. Notable features used by this site are:</p> <ul> <li>Blog plugin</li> <li>Privacy plugin</li> <li>Grid cards</li> <li>Automatic light / dark mode</li> <li>Rich search previews</li> </ul>"},{"location":"guides/mkdocs-material-insider-github-pages-action/#getting-started","title":"Getting Started","text":"<p>You will need Python 3.10 (I strongly recommend using pyenv), Poetry and a GitHub repository. If you are on macOS, check out my guide on macOS Setup to get your development environment up and running.</p>"},{"location":"guides/mkdocs-material-insider-github-pages-action/#poetry-init","title":"Poetry Init","text":"<p>First you need to create a <code>pyproject.toml</code> file, which will define the packages uses for building the site. This can be done interactively by running <code>poetry init</code> in the base folder of your repo.</p> <pre><code>poetry init\n</code></pre> <p>The output should look something like below. Don't define your dependencies interactively, you'll setup a GitHub token in later steps which will let us access the Insiders repo. For the rest of the prompts, the defaults are typically good enough.</p> <pre><code>$ poetry init\n\nThis command will guide you through creating your pyproject.toml config.\n\nPackage name [website-test]:  \nVersion [0.1.0]:  \nDescription []:  \nAuthor [Andrew LeCody &lt;andrewlecody@gmail.com&gt;, n to skip]:  \nLicense []:  \nCompatible Python versions [^3.10]:  \n\nWould you like to define your main dependencies interactively? (yes/no) [yes] no\nWould you like to define your development dependencies interactively? (yes/no) [yes] no\nGenerated file\n\n[tool.poetry]\nname = \"website-test\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Andrew LeCody &lt;andrewlecody@gmail.com&gt;\"]\nreadme = \"README.md\"\npackages = [{include = \"website_test\"}]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\n\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\nDo you confirm generation? (yes/no) [yes]\n</code></pre>"},{"location":"guides/mkdocs-material-insider-github-pages-action/#github-token","title":"GitHub Token","text":"<p>You need to create a Personal Access Token and set it as the environment variable <code>$GH_TOKEN</code>.</p> <ol> <li>Go to https://github.com/settings/tokens</li> <li>Click on \"Generate a new token\" in the top right.</li> <li>Enter a name for your token and set the expiration. You'll have to generate a new token once this one expires.</li> <li>Select just the <code>repo</code> scope.</li> <li>Generate the token, on the next page you need to copy the token (it should start with <code>ghp_</code>).</li> <li> <p>Edit the settings file for your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code>)</p> <pre><code>export GH_TOKEN=\"paste_token_here\"\n</code></pre> </li> <li> <p>Reload your shell (or just run that <code>export</code> command) so that the <code>$GH_TOKEN</code> environment variable is active.</p> </li> <li>You can verify the variable is loaded by running <code>echo $GH_TOKEN</code>, if your token appears, you are ready to continue.</li> </ol>"},{"location":"guides/mkdocs-material-insider-github-pages-action/#configure-poetry","title":"Configure Poetry","text":"<p>Poetry needs to be told how to access the Insiders repo, using the GitHub token you generated.</p> <pre><code>poetry config repositories.github-squidfunk-mkdocs https://github.com/squidfunk/mkdocs-material-insiders.git\npoetry config http-basic.github-squidfunk-mkdocs username $GH_TOKEN\n</code></pre> <p>Now you are ready to install Material for MkDocs Insiders.</p> <pre><code>poetry add git+https://github.com/squidfunk/mkdocs-material-insiders.git\n</code></pre> <p>This is optional, but I install some additional packages. I use the minify plugin to shrink the built HTML files, while Pillow and CairoSVG are needed for the social media cards.</p> <pre><code>poetry add mkdocs-minify-plugin Pillow CairoSVG\n</code></pre>"},{"location":"guides/mkdocs-material-insider-github-pages-action/#build-your-site","title":"Build Your Site","text":"<p>There are a lot of great guides out there on building a site with MkDocs so I won't get into it. Feel free to check out my repo on GitHub to see how I have this site setup: https://github.com/aceat64/website</p> <p>Since mkdocs was installed via Poetry, you'll need to call it with <code>poetry run mkdocs</code>. For example, to run the development server:</p> <pre><code>poetry run mkdocs serve\n</code></pre>"},{"location":"guides/mkdocs-material-insider-github-pages-action/#github-pages","title":"GitHub Pages","text":"<p>If you haven't already, go into the settings for you GitHub repo and enable Pages. Under \"Build and deployment\" you'll want to select \"GitHub Actions\" as the source.</p>"},{"location":"guides/mkdocs-material-insider-github-pages-action/#github-actions","title":"GitHub Actions","text":"<p>All that's left now is to configure a workflow to build and publish the site. Place the following file in your repo, commit your changes, push to GitHub and sit back!</p> .github/workflows/pages.yaml<pre><code>name: Pages\non:\npush:\nbranches:\n- main\n# workflow_dispatch allows the site to be rebuilt and published manually if needed\nworkflow_dispatch:\n\n# Grant GITHUB_TOKEN the permissions required to make a Pages deployment\npermissions:\npages: write # to deploy to Pages\nid-token: write # to verify the deployment originates from an appropriate source\n\nconcurrency: pages-build-and-deploy\n\njobs:\nbuild:\nruns-on: ubuntu-latest\n# Running this action on a fork will likely fail anyway\n# unless the forked repo also has access to material for mkdocs insiders\nif: github.event.repository.fork == false\nsteps:\n- uses: actions/checkout@v3\n\n# The .cache directory is used for 3rd party assets, as part of the privacy plugin.\n# It is also used to cache the generated social media cards.\n# Persisting the cache across builds dramatically speeds up the process.\n- name: Load site cache\nuses: actions/cache@v3\nwith:\nkey: ${{ github.ref }}\npath: .cache\n\n# We use poetry to manage mkdocs, plugins, etc\n- name: Install poetry\nrun: pipx install poetry\n\n- name: Setup Python\nid: setup-python\nuses: actions/setup-python@v4\nwith:\npython-version: \"3.11\"\n# This ensures the package cache for poetry/pip is persisted, again speeding up this build action.\ncache: \"poetry\"\n\n# Insiders is the sponsorware version of Material for MkDocs\n# https://squidfunk.github.io/mkdocs-material/insiders/\n- name: Configure auth for Insiders\nrun: |\npoetry config repositories.github-squidfunk-mkdocs https://github.com/squidfunk/mkdocs-material-insiders.git\npoetry config http-basic.github-squidfunk-mkdocs username ${{ secrets.GH_TOKEN }}\n- name: Install deps\nrun: poetry install\n- name: Build site\nrun: poetry run mkdocs build\n\n# Upload the built site as an artifact, this will be used by the deploy job.\n- uses: actions/upload-pages-artifact@v1\nwith:\npath: \"site\"\n\ndeploy:\nneeds: build\n\n# Deploy to the github-pages environment\nenvironment:\nname: github-pages\nurl: ${{ steps.deployment.outputs.page_url }}\n\n# Specify runner + deployment step\nruns-on: ubuntu-latest\nsteps:\n- name: Deploy to GitHub Pages\nid: deployment\nuses: actions/deploy-pages@v1\n</code></pre>"},{"location":"guides/nginx-proxy-manager-udmp/","title":"Nginx Proxy Manager on UDM-Pro","text":"<p>https://nginxproxymanager.com/</p>"},{"location":"guides/nginx-proxy-manager-udmp/#credits","title":"Credits","text":"<p>https://github.com/unifi-utilities/unifios-utilities/issues/149</p>"},{"location":"guides/nginx-proxy-manager-udmp/#requirements","title":"Requirements","text":"<ol> <li>Setup on-boot-script from unifi-utilities/unifios-utilities.</li> <li>Install the container-common scripts. This will keep the container logs from growing too large.</li> </ol>"},{"location":"guides/nginx-proxy-manager-udmp/#setup","title":"Setup","text":"<ul> <li>In your UniFi Network controller, create <code>proxymanager</code> network (vlan 6, 10.0.6.1/24, no DHCP).</li> <li> <p>Create directories:</p> <pre><code>mkdir -p /mnt/data/proxymanager/data /mnt/data/proxymanager/letsencrypt\n</code></pre> </li> <li> <p>Create <code>/mnt/data/podman/cni/30-proxymanager.conflist</code> with the following:</p> </li> </ul> 30-proxymanager.conflist <pre><code>{\n\"cniVersion\": \"0.4.0\",\n\"name\": \"proxymanager\",\n\"plugins\": [\n{\n\"type\": \"macvlan\",\n\"mode\": \"bridge\",\n\"master\": \"br6\",\n\"ipam\": {\n\"type\": \"static\",\n\"addresses\": [\n{\n\"address\": \"10.0.6.4/24\",\n\"gateway\": \"10.0.6.1\"\n}\n],\n\"routes\": [\n{\"dst\": \"0.0.0.0/0\"}\n]\n}\n}\n]\n}\n</code></pre> <ul> <li>Create <code>/mnt/data/on_boot.d/20-proxymanager.sh</code> with the following:</li> </ul> 20-proxymanager.sh <pre><code>#!/bin/sh\n\n## configuration variables\nVLAN=6\nIPV4_IP=\"10.0.6.4\"\n\n# This is the IP address of the container. You may want to set it to match\n# your own network structure such as 192.168.5.3 or similar\nIPV4_GW=\"10.0.6.1/24\"\n\n# As above, this should match the gateway of the VLAN for the container\n# network as above which is usually the .1/24 range of the IPV4_IP\n# container name; e.g. nextdns, pihole, adguardhome, etc\nCONTAINER=proxymanager\n\nif ! test -f /opt/cni/bin/macvlan; then\necho \"Error: CNI plugins not found.\" &gt;&amp;2\nexit 1\nfi\n\n# set VLAN bridge promiscuous\nip link set br${VLAN} promisc on\n\n# create macvlan bridge and add IPv4 IP\nip link add br${VLAN}.mac link br${VLAN} type macvlan mode bridge\nip addr add ${IPV4_GW} dev br${VLAN}.mac noprefixroute\n\n# (optional) add IPv6 IP to VLAN bridge macvlan bridge\nif [ -n \"${IPV6_GW}\" ]; then\nip -6 addr add ${IPV6_GW} dev br${VLAN}.mac noprefixroute\nfi\n\n# set macvlan bridge promiscuous and bring it up\nip link set br${VLAN}.mac promisc on\nip link set br${VLAN}.mac up\n\n# add IPv4 route to DNS container\nip route add ${IPV4_IP}/32 dev br${VLAN}.mac\n\n# (optional) add IPv6 route to container\nif [ -n \"${IPV6_IP}\" ]; then\nip -6 route add ${IPV6_IP}/128 dev br${VLAN}.mac\nfi\n\nif podman container exists ${CONTAINER}; then\npodman start ${CONTAINER}\nelse\nlogger -s -t podman-dns -p ERROR Container $CONTAINER not found, make sure you set the proper name, you can ignore this error if it is your first time setting it up\nfi\n</code></pre> <ul> <li>Run the following commands:</li> </ul> <pre><code>chmod +x /mnt/data/on_boot.d/20-proxymanager.sh\n/mnt/data/on_boot.d/20-proxymanager.sh\n/mnt/data/on_boot.d/05-install-cni-plugins.sh\n</code></pre> <ul> <li>Start NPM:</li> </ul> <pre><code>podman run -d \\\n--systemd=false \\\n--network proxymanager \\\n--name proxymanager \\\n-e TZ=America/Chicago \\\n-e DB_SQLITE_FILE=\"/data/database.sqlite\" \\\n-v \"/mnt/data/proxymanager/data:/data\" \\\n-v \"/mnt/data/proxymanager/letsencrypt:/etc/letsencrypt\" \\\njc21/nginx-proxy-manager:latest\n</code></pre> <p>If everything worked the Nginx Proxy Manager interface should be available at http://10.0.6.4:81</p> <p>Default Admin User:</p> <pre><code>admin@example.com\n</code></pre> <p>Default Admin Password:</p> <pre><code>changeme\n</code></pre>"},{"location":"guides/rpi-k8s-talos-terraform/","title":"K8s on RPI with Talos and Terraform","text":"<p> View repo</p> <p>This repo is a fully working example of deploying a Kubernetes cluster to a handful of Raspberry Pi 4s. We use Talos for the OS and building the Kubernetes cluster. After the cluster is bootstraped, we use Terraform deploy various useful services onto the cluster.</p>"},{"location":"guides/rpi-k8s-talos-terraform/#features","title":"Features","text":"<p>At the end of this tutorial you'll have a fully working Kubernetes cluster with the following services configured and ready to use:</p> <ul> <li>Talos - Minimal and hardened operating system and tools that deploy and manage kubernetes nodes/clusters.<ul> <li>Virtual (shared) IP address for the talos and Kubernetes endpoints</li> </ul> </li> <li>MetalLB - Load balancers using virtual/shared IPs</li> <li>metrics-server - Provide metrics for Kubernetes autoscaling (e.g. horizontal pod autoscaler)</li> <li>cert-manager - Automated TLS certificate management</li> <li>Rook-Ceph - Distributed block, object and file storage</li> <li>Prometheus - Monitoring and alerting<ul> <li>Full monitoring of your cluster! We gather metrics from just about every service that has them.</li> </ul> </li> <li>Loki - Log aggregation</li> <li>Grafana - Visualize and explore metrics, logs and other data.<ul> <li>Since we use the kube-prometheus-stack  helm chart a bunch of dashboards are pre-generated for you. We also automatically deploy dashboards for monitoring rook-ceph.</li> </ul> </li> <li>Alertmanager - Send Prometheus alerts to email, PagerDuty, etc.</li> <li>Vertical Pod Autoscaler - Suggest or automatically adjust resource limits and requests for pods.</li> </ul>"},{"location":"guides/rpi-k8s-talos-terraform/#helm-charts-and-terraform-modules-used","title":"Helm Charts and Terraform Modules Used","text":"<ul> <li>cert-manager v1.3.1</li> <li>metrics-server 2.11.4</li> <li>vpa 0.3.2</li> <li>loki-stack 2.3.1</li> <li>kube-prometheus-stack 15.3.1</li> <li>rook-ceph v1.6.1</li> <li>terraform-kubernetes-metallb v0.1.6</li> </ul>"},{"location":"literary/recommended-reading/","title":"Recommended Reading","text":"<p>These are some of the books I recommend reading. I prefer buying physical books from indepenant/local book stores and from online retailers/publishers that offer DRM-free ebooks. If you have to ask Amazon before you open a book, you don't own that book, Amazon does.</p> <p>No Starch Press is great, when you purchase a printed book you get a DRM-free ebook as well. These are always in PDF format but sometimes also in Mobi and ePub too. They also offer \"early access\" to chapters of books you've pre-ordered.</p> <p>This list is always growing, and it's nowhere complete, so check back from time to time for more.</p>"},{"location":"literary/recommended-reading/#manga-guides","title":"Manga Guides","text":"<p>The \"Manga Guide\" series is a great way to learn about various topics from databases to physics. You can find them all on No Starch Press. If you are a developer and work with any kind of relational database, I strongly recommend getting The Manga Guide to Databases, especially if you have never heard of normalization.</p>"},{"location":"literary/recommended-reading/#engineering","title":"Engineering","text":"<ul> <li>Engineering in Plain Sight by Grady Hillhouse of Practical Engineering on YouTube.</li> </ul>"},{"location":"literary/recommended-reading/#sci-fi","title":"Sci-Fi","text":"<ul> <li>Literally anything by qntm (Sam Hughes), and all of it is available online for free.<ul> <li>There Is No Antimemetics Division - Set in the world of the SCP Foundation wiki. It explores the concept of antimemes, ideas with self-censoring properties.</li> <li>Ra - Magic as science/engineering, think \"magic NASA\".</li> <li>Fine Structure - The first thing I ever read from qntm, I have been a fan ever since.</li> <li>Ed - Hilarious and over the top mad science. Who doesn't like giant robot battles?</li> </ul> </li> </ul>"},{"location":"projects/battery-backup/","title":"Battery Backup","text":"<p>https://talk.dallasmakerspace.org/t/buildlog-home-battery-backup/86488</p>"},{"location":"projects/jeep/","title":"Jeep","text":""},{"location":"smart-home/architecture/","title":"Architecture","text":""},{"location":"smart-home/garage-door/","title":"Garage Door","text":""},{"location":"smart-home/garage-door/#materials","title":"Materials","text":"<ul> <li>Adafruit HUZZAH32 \u2013 ESP32 Feather Board</li> <li>Adafruit Non-Latching Mini Relay FeatherWing</li> <li>Magnetic door sensor or reed switch. Some examples:<ul> <li>Magnetic contact switch (door sensor)</li> <li>Heavy Duty Wired Alarm Garage Door Magnetic Contacts Switch Sensor NC with Adjustable Bracket</li> </ul> </li> </ul>"},{"location":"smart-home/garage-door/#esphome-config","title":"ESPHome config","text":"<pre><code>substitutions:\ndevice_name: garage-door\nfriendly_name: Garage Door\n\nesphome:\nname: ${device_name}\ncomment: Open and close a garage door\nproject:\nname: \"aceat64.garage_door\"\nversion: \"1.0.0\"\n\nesp32:\nboard: featheresp32\n\nwifi:\n# Use a static IP, it improves connection time\n# Recommended by ESPHome: https://esphome.io/components/wifi.html\nmanual_ip:\nstatic_ip: 192.168.xxx.xxx\ngateway: 192.168.xxx.xxx\nsubnet: 255.255.255.0\ndns1: 192.168.xxx.xxx\n# SSID and password stored as secrets, so that all devices share the config\nssid: !secret wifi_ssid\npassword: !secret wifi_password\n\n# Enable fallback hotspot (captive portal) in case wifi connection fails\nap:\nssid: ${device_name}\npassword: !secret fallback_password\n\ncaptive_portal:\n\n# Enable logging\nlogger:\n# Make sure logging is not using the serial port\nbaud_rate: 0\n\n# Enable Home Assistant API\napi:\n\nota:\npassword: !secret ota_password\n\nbinary_sensor:\n- platform: gpio\npin:\nnumber: A0\nmode: INPUT_PULLUP\nname: \"Garage Door Contact Sensor\"\nid: contact_sensor\ninternal: true\n\nswitch:\n- platform: gpio\npin: A1\nname: \"Garage Door Relay\"\nid: relay\ninternal: true\n\ncover:\n- platform: template\ndevice_class: garage\nname: ${friendly_name}\nlambda: |-\nif (id(contact_sensor).state) {\nreturn COVER_OPEN;\n} else {\nreturn COVER_CLOSED;\n}\nopen_action:\n- switch.turn_on: relay\n- delay: 0.5s\n- switch.turn_off: relay\nclose_action:\n- switch.turn_on: relay\n- delay: 0.5s\n- switch.turn_off: relay\n</code></pre>"},{"location":"smart-home/switches/","title":"Light/Fan Switches","text":"<p>In our house, all of the light and fan switches are \"Martin Jerry Smart Switches\", as they use the common Tuya module, which is really just an esp8266 board. Perfect for reflashing and running ESPHome on!</p>"},{"location":"smart-home/switches/#switch","title":"Switch","text":"<pre><code>substitutions:\ndevice_name: hall-bath-light\nfriendly_name: Hall Bath Light\n\nesphome:\nname: ${device_name}\ncomment: On/off light switch\nproject:\nname: \"aceat64.light_switch\"\nversion: \"1.0.0\"\n\nesp8266:\n# Switch uses a Tuya module\nboard: esp01_1m\n# Keeps the light's state saved in case of power loss\nrestore_from_flash: true\n\nwifi:\n# Use a static IP, it improves connection time\n# Recommended by ESPHome: https://esphome.io/components/wifi.html\nmanual_ip:\nstatic_ip: 192.168.xxx.xxx\ngateway: 192.168.xxx.xxx\nsubnet: 255.255.255.0\ndns1: 192.168.xxx.xxx\n# SSID and password stored as secrets, so that all devices share the config\nssid: !secret wifi_ssid\npassword: !secret wifi_password\n\n# Enable fallback hotspot (captive portal) in case wifi connection fails\nap:\nssid: ${device_name}\npassword: !secret fallback_password\n\ncaptive_portal:\n\n# Enable logging\nlogger:\n# Make sure logging is not using the serial port\nbaud_rate: 0\n\n# Enable Home Assistant API\napi:\n\nota:\npassword: !secret ota_password\n\noutput:\n- platform: gpio\npin: GPIO12\nid: power\n- platform: gpio\npin: GPIO5\ninverted: true\nid: led1\n\nlight:\n- platform: binary\nname: ${friendly_name}\noutput: power\nid: powerswitch\non_turn_on:\n- output.turn_on: led1\non_turn_off:\n- output.turn_off: led1\n\nstatus_led:\npin:\nnumber: GPIO4\ninverted: true\n\nbinary_sensor:\n- platform: gpio\npin:\nnumber: GPIO13\nmode: INPUT_PULLUP\ninternal: true\nid: main_button\non_press:\n- light.toggle: powerswitch\n</code></pre>"},{"location":"smart-home/switches/#dimmer","title":"Dimmer","text":"<pre><code># Based on: https://github.com/mjoshd/esphome_martin-jerry-mj-sd01-dimmer/blob/master/martin_jerry_mj_sd01_dimmer.yaml\nsubstitutions:\ndevice_name: hall-light\nfriendly_name: Hall Light\npwm_min_power: 15% # keep dimming functional at lowest levels\nno_delay: 0s # transition when changing dimmer_lvl &amp; relay delay\ntransition_length: .5s # transition when turning on/off\nlong_press_min: .4s # minimum time to activate long-press action\nlong_press_max: 2s # maximum time to activate long-press action\nlong_press_up: 100% # long press brightness\nlong_press_down: 33% # long press brightness\nlong_press_main: 50% # long press brightness\n\nesphome:\nname: ${device_name}\ncomment: Dimmer switch\nproject:\nname: \"aceat64.dimmer_switch\"\nversion: \"1.0.0\"\n\nesp8266:\n# Switch uses a Tuya module\nboard: esp01_1m\n# Keeps the light's state saved in case of power loss\nrestore_from_flash: true\n\nwifi:\n# Use a static IP, it improves connection time\n# Recommended by ESPHome: https://esphome.io/components/wifi.html\nmanual_ip:\nstatic_ip: 192.168.xxx.xxx\ngateway: 192.168.xxx.xxx\nsubnet: 255.255.255.0\ndns1: 192.168.xxx.xxx\n# SSID and password stored as secrets, so that all devices share the config\nssid: !secret wifi_ssid\npassword: !secret wifi_password\n\n# Enable fallback hotspot (captive portal) in case wifi connection fails\nap:\nssid: ${device_name}\npassword: !secret fallback_password\n\ncaptive_portal:\n\n# Enable logging\nlogger:\n# Make sure logging is not using the serial port\nbaud_rate: 0\n\n# Enable Home Assistant API\napi:\n\nota:\npassword: !secret ota_password\n\noutput:\n- platform: gpio\npin: GPIO3\ninverted: true\nid: led5\n- platform: gpio\npin: GPIO5\ninverted: true\nid: led4\n- platform: gpio\npin: GPIO12\ninverted: true\nid: led3\n- platform: gpio\npin: GPIO14\ninverted: true\nid: led2\n- platform: esp8266_pwm\npin: GPIO13\nid: pwm\npower_supply: relay\nmin_power: ${pwm_min_power}\n\nlight:\n- platform: monochromatic\nname: ${friendly_name}\noutput: pwm\ndefault_transition_length: ${no_delay}\nid: dimmer\n\nstatus_led:\npin:\nnumber: GPIO4\ninverted: true\n\npower_supply:\n- id: relay\npin:\nnumber: GPIO16\ninverted: True\nenable_time: ${no_delay}\nkeep_on_time: ${no_delay}\n\nbinary_sensor:\n- platform: gpio\npin:\nnumber: GPIO0\ninverted: True\nmode: INPUT_PULLUP\nname: ${friendly_name} Up Button\nid: up_button\ninternal: True\non_press:\nthen:\n- lambda: |-\nif (id(dimmer_lvl) &gt; .91) {\nid(dimmer_lvl) = 1.0;\n}\nelse if (id(dimmer_lvl) &lt;= .91) {\nid(dimmer_lvl) += .083;\n};\nid(apply_dimming).execute();\non_click:\nmin_length: ${long_press_min}\nmax_length: ${long_press_max}\nthen:\n- light.turn_on:\nid: dimmer\nbrightness: ${long_press_up}\n- platform: gpio\npin:\nnumber: GPIO1\ninverted: True\nmode: INPUT_PULLUP\nname: ${friendly_name} Down Button\ninternal: True\non_press:\nthen:\n- lambda: !lambda |-\nif (id(dimmer_lvl) &lt; .10) {\nid(dimmer_lvl) = .01;\n}\nelse if (id(dimmer_lvl) &gt;= .10) {\nid(dimmer_lvl) -= .083;\n};\nid(apply_dimming).execute();\non_click:\nmin_length: ${long_press_min}\nmax_length: ${long_press_max}\nthen:\n- light.turn_on:\nid: dimmer\nbrightness: ${long_press_down}\n- platform: gpio\npin:\nnumber: GPIO15\nmode: INPUT_PULLUP\nname: ${friendly_name} Main Button\ninternal: True\non_press:\n- light.toggle: dimmer\non_click:\nmin_length: ${long_press_min}\nmax_length: ${long_press_max}\nthen:\n- light.turn_on:\nid: dimmer\nbrightness: ${long_press_main}\n\nglobals:\n- id: dimmer_lvl\ntype: float\nrestore_value: no\ninitial_value: \"1.0\"\n\nscript:\n- id: apply_dimming\nthen:\n- lambda: |-\nauto call = id(dimmer).turn_on();\ncall.set_brightness(id(dimmer_lvl));\ncall.perform();\n- logger.log:\nformat: \"dimmer_lvl = %.2f\"\nargs: [\"id(dimmer_lvl)\"]\n\ninterval:\n- interval: 250ms\nthen:\n- lambda: |-\nauto dimmer_vals = id(dimmer).current_values;\nif (dimmer_vals.is_on()) {\nid(dimmer_lvl) = dimmer_vals.get_brightness();\n}\nif (id(dimmer_lvl) &gt; .19) { id(led2).turn_on(); }\nif (id(dimmer_lvl) &lt; .20) { id(led2).turn_off(); }\nif (id(dimmer_lvl) &gt; .39) { id(led3).turn_on(); }\nif (id(dimmer_lvl) &lt; .40) { id(led3).turn_off(); }\nif (id(dimmer_lvl) &gt; .59) { id(led4).turn_on(); }\nif (id(dimmer_lvl) &lt; .60) { id(led4).turn_off(); }\nif (id(dimmer_lvl) &gt; .79) { id(led5).turn_on(); }\nif (id(dimmer_lvl) &lt; .80) { id(led5).turn_off(); }\nif (!dimmer_vals.is_on()) {\nid(led2).turn_off();\nid(led3).turn_off();\nid(led4).turn_off();\nid(led5).turn_off();\n}\n</code></pre>"},{"location":"blog/page/2/","title":"Index","text":""}]}